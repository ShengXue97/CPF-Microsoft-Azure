{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tkinter as tk\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['keywords', 'urgency']\n",
    "rows = []\n",
    "\n",
    "training_data = pd.DataFrame(rows, columns=columns)\n",
    "training_data = pd.read_csv(\"cpf_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1_docs = [row['keywords'] for index,row in training_data.iterrows() if row['urgency'] == 1]\n",
    "\n",
    "vec_u1 = CountVectorizer()\n",
    "X_u1 = vec_u1.fit_transform(u1_docs)\n",
    "tdm_u1 = pd.DataFrame(X_u1.toarray(), columns=vec_u1.get_feature_names())\n",
    "\n",
    "u2_docs = [row['keywords'] for index,row in training_data.iterrows() if row['urgency'] == 2]\n",
    "\n",
    "vec_u2 = CountVectorizer()\n",
    "X_u2 = vec_u2.fit_transform(u2_docs)\n",
    "tdm_u2 = pd.DataFrame(X_u2.toarray(), columns=vec_u2.get_feature_names())\n",
    "\n",
    "u3_docs = [row['keywords'] for index,row in training_data.iterrows() if row['urgency'] == 3]\n",
    "\n",
    "vec_u3 = CountVectorizer()\n",
    "X_u3 = vec_u3.fit_transform(u3_docs)\n",
    "tdm_u3 = pd.DataFrame(X_u3.toarray(), columns=vec_u3.get_feature_names())\n",
    "\n",
    "u4_docs = [row['keywords'] for index,row in training_data.iterrows() if row['urgency'] == 4]\n",
    "\n",
    "vec_u4 = CountVectorizer()\n",
    "X_u4 = vec_u4.fit_transform(u4_docs)\n",
    "tdm_u4 = pd.DataFrame(X_u4.toarray(), columns=vec_u4.get_feature_names())\n",
    "\n",
    "u5_docs = [row['keywords'] for index,row in training_data.iterrows() if row['urgency'] == 5]\n",
    "\n",
    "vec_u5 = CountVectorizer()\n",
    "X_u5 = vec_u5.fit_transform(u5_docs)\n",
    "tdm_u5 = pd.DataFrame(X_u5.toarray(), columns=vec_u5.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of words for each urgency label\n",
    "\n",
    "word_list_u1 = vec_u1.get_feature_names();    \n",
    "count_list_u1 = X_u1.toarray().sum(axis=0) \n",
    "freq_u1 = dict(zip(word_list_u1,count_list_u1))\n",
    "\n",
    "word_list_u2 = vec_u2.get_feature_names();    \n",
    "count_list_u2 = X_u2.toarray().sum(axis=0) \n",
    "freq_u2 = dict(zip(word_list_u2,count_list_u2))\n",
    "\n",
    "word_list_u3 = vec_u3.get_feature_names();    \n",
    "count_list_u3 = X_u3.toarray().sum(axis=0) \n",
    "freq_u3 = dict(zip(word_list_u3,count_list_u3))\n",
    "\n",
    "word_list_u4 = vec_u4.get_feature_names();    \n",
    "count_list_u4 = X_u4.toarray().sum(axis=0) \n",
    "freq_u4 = dict(zip(word_list_u4,count_list_u4))\n",
    "\n",
    "word_list_u5 = vec_u5.get_feature_names();    \n",
    "count_list_u5 = X_u5.toarray().sum(axis=0) \n",
    "freq_u5 = dict(zip(word_list_u5,count_list_u5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write values to CSV file\n",
    "import csv\n",
    "\n",
    "with open('u1_word_frequencies.csv', 'w') as f:\n",
    "    for key in freq_u1.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,freq_u1[key]))\n",
    "        \n",
    "with open('u2_word_frequencies.csv', 'w') as f:\n",
    "    for key in freq_u2.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,freq_u2[key]))\n",
    "        \n",
    "with open('u3_word_frequencies.csv', 'w') as f:\n",
    "    for key in freq_u3.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,freq_u3[key]))\n",
    "\n",
    "with open('u4_word_frequencies.csv', 'w') as f:\n",
    "    for key in freq_u4.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,freq_u4[key]))\n",
    "        \n",
    "with open('u5_word_frequencies.csv', 'w') as f:\n",
    "    for key in freq_u5.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,freq_u5[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Laplace Smoothing\n",
    "\n",
    "docs = [row['keywords'] for index,row in training_data.iterrows()]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(docs)\n",
    "\n",
    "total_features = len(vec.get_feature_names())\n",
    "\n",
    "total_cnts_features_u1 = count_list_u1.sum(axis=0)\n",
    "total_cnts_features_u2 = count_list_u2.sum(axis=0)\n",
    "total_cnts_features_u3 = count_list_u3.sum(axis=0)\n",
    "total_cnts_features_u4 = count_list_u4.sum(axis=0)\n",
    "total_cnts_features_u5 = count_list_u4.sum(axis=0)\n",
    "\n",
    "features_dict = {}\n",
    "features_dict['total'] = total_features\n",
    "features_dict['u1'] = total_cnts_features_u1\n",
    "features_dict['u2'] = total_cnts_features_u2\n",
    "features_dict['u3'] = total_cnts_features_u3\n",
    "features_dict['u4'] = total_cnts_features_u4\n",
    "features_dict['u5'] = total_cnts_features_u5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_cnts_features.csv', 'w') as f:\n",
    "    for key in features_dict.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,features_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1_prob = training_data.groupby('urgency').count()['keywords'][1]\n",
    "u2_prob = training_data.groupby('urgency').count()['keywords'][2]\n",
    "u3_prob = training_data.groupby('urgency').count()['keywords'][3]\n",
    "u4_prob = training_data.groupby('urgency').count()['keywords'][4]\n",
    "u5_prob = training_data.groupby('urgency').count()['keywords'][5]\n",
    "\n",
    "u_probabilities = {}\n",
    "u_probabilities['u1'] = u1_prob\n",
    "u_probabilities['u2'] = u2_prob\n",
    "u_probabilities['u3'] = u3_prob\n",
    "u_probabilities['u4'] = u4_prob\n",
    "u_probabilities['u5'] = u5_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('u_probabilities.csv', 'w') as f:\n",
    "    for key in u_probabilities.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,u_probabilities[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
