{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\simsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tkinter as tk\n",
    "import requests\n",
    "# pprint is used to format the JSON response\n",
    "from pprint import pprint\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "subscription_key = \"cd0cf9855b244aa28c017742ed7a904c\"\n",
    "endpoint = \"https://cpftext.cognitiveservices.azure.com/\"\n",
    "\n",
    "sentiment_url = endpoint + \"/text/analytics/v2.1/sentiment\"\n",
    "language_api_url = endpoint + \"/text/analytics/v2.1/languages\"\n",
    "keyphrase_url = endpoint + \"/text/analytics/v2.1/keyphrases\"\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "columns = ['keywords', 'urgency']\n",
    "rows = []\n",
    "\n",
    "training_data = pd.DataFrame(rows, columns=columns)\n",
    "training_data = pd.read_csv(\"cpf_processed.csv\")\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "u1_docs = [row['keywords'] for index,row in training_data.iterrows() if row['urgency'] == 1]\n",
    "\n",
    "vec_u1 = CountVectorizer()\n",
    "\n",
    "X_u1 = vec_u1.fit_transform(u1_docs)\n",
    "tdm_u1 = pd.DataFrame(X_u1.toarray(), columns=vec_u1.get_feature_names())\n",
    "\n",
    "u2_docs = [row['keywords'] for index,row in training_data.iterrows() if row['urgency'] == 2]\n",
    "\n",
    "vec_u2 = CountVectorizer()\n",
    "X_u2 = vec_u2.fit_transform(u2_docs)\n",
    "tdm_u2 = pd.DataFrame(X_u2.toarray(), columns=vec_u2.get_feature_names())\n",
    "\n",
    "u3_docs = [row['keywords'] for index,row in training_data.iterrows() if row['urgency'] == 3]\n",
    "\n",
    "vec_u3 = CountVectorizer()\n",
    "X_u3 = vec_u3.fit_transform(u3_docs)\n",
    "tdm_u3 = pd.DataFrame(X_u3.toarray(), columns=vec_u3.get_feature_names())\n",
    "\n",
    "u4_docs = [row['keywords'] for index,row in training_data.iterrows() if row['urgency'] == 4]\n",
    "\n",
    "vec_u4 = CountVectorizer()\n",
    "X_u4 = vec_u4.fit_transform(u4_docs)\n",
    "tdm_u4 = pd.DataFrame(X_u4.toarray(), columns=vec_u4.get_feature_names())\n",
    "\n",
    "u5_docs = [row['keywords'] for index,row in training_data.iterrows() if row['urgency'] == 5]\n",
    "\n",
    "vec_u5 = CountVectorizer()\n",
    "X_u5 = vec_u5.fit_transform(u5_docs)\n",
    "tdm_u5 = pd.DataFrame(X_u5.toarray(), columns=vec_u5.get_feature_names())\n",
    "\n",
    "##\n",
    "\n",
    "word_list_u1 = vec_u1.get_feature_names();    \n",
    "count_list_u1 = X_u1.toarray().sum(axis=0) \n",
    "freq_u1 = dict(zip(word_list_u1,count_list_u1))\n",
    "\n",
    "word_list_u2 = vec_u2.get_feature_names();    \n",
    "count_list_u2 = X_u2.toarray().sum(axis=0) \n",
    "freq_u2 = dict(zip(word_list_u2,count_list_u2))\n",
    "\n",
    "word_list_u3 = vec_u3.get_feature_names();    \n",
    "count_list_u3 = X_u3.toarray().sum(axis=0) \n",
    "freq_u3 = dict(zip(word_list_u3,count_list_u3))\n",
    "\n",
    "word_list_u4 = vec_u4.get_feature_names();    \n",
    "count_list_u4 = X_u4.toarray().sum(axis=0) \n",
    "freq_u4 = dict(zip(word_list_u4,count_list_u4))\n",
    "\n",
    "word_list_u5 = vec_u5.get_feature_names();    \n",
    "count_list_u5 = X_u5.toarray().sum(axis=0) \n",
    "freq_u5 = dict(zip(word_list_u5,count_list_u5))\n",
    "\n",
    "##\n",
    "\n",
    "prob_u1 = []\n",
    "for word, count in zip(word_list_u1, count_list_u1):\n",
    "    prob_u1.append(count/sum(freq_u1.values()))\n",
    "dict(zip(word_list_u1, prob_u1))\n",
    "\n",
    "prob_u2 = []\n",
    "for word, count in zip(word_list_u2, count_list_u2):\n",
    "    prob_u2.append(count/sum(freq_u2.values()))\n",
    "dict(zip(word_list_u2, prob_u2))\n",
    "\n",
    "prob_u3 = []\n",
    "for word, count in zip(word_list_u3, count_list_u3):\n",
    "    prob_u3.append(count/sum(freq_u3.values()))\n",
    "dict(zip(word_list_u3, prob_u3))\n",
    "\n",
    "prob_u4 = []\n",
    "for word, count in zip(word_list_u4, count_list_u4):\n",
    "    prob_u4.append(count/sum(freq_u4.values()))\n",
    "dict(zip(word_list_u4, prob_u4))\n",
    "\n",
    "prob_u5 = []\n",
    "for word, count in zip(word_list_u5, count_list_u5):\n",
    "    prob_u5.append(count/sum(freq_u5.values()))\n",
    "dict(zip(word_list_u5, prob_u5))\n",
    "\n",
    "##\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = [row['keywords'] for index,row in training_data.iterrows()]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(docs)\n",
    "\n",
    "total_features = len(vec.get_feature_names())\n",
    "\n",
    "total_cnts_features_u1 = count_list_u1.sum(axis=0)\n",
    "total_cnts_features_u2 = count_list_u2.sum(axis=0)\n",
    "total_cnts_features_u3 = count_list_u3.sum(axis=0)\n",
    "total_cnts_features_u4 = count_list_u4.sum(axis=0)\n",
    "total_cnts_features_u5 = count_list_u4.sum(axis=0)\n",
    "\n",
    "\n",
    "def predict_urgency(sentence):\n",
    "    new_word_list = word_tokenize(sentence)\n",
    "\n",
    "    prob_u1_with_ls = []\n",
    "    for word in new_word_list:\n",
    "        if word in freq_u1.keys():\n",
    "            count = freq_u1[word]\n",
    "        else:\n",
    "            count = 0\n",
    "        prob_u1_with_ls.append((count + 1)/(total_cnts_features_u1 + total_features))\n",
    "    u1_dict = dict(zip(new_word_list,prob_u1_with_ls))\n",
    "    u1_prob = training_data.groupby('urgency').count()['keywords'][1]\n",
    "    for keyword in new_word_list:\n",
    "        u1_prob = u1_prob * u1_dict[keyword]\n",
    "\n",
    "    prob_u2_with_ls = []\n",
    "    for word in new_word_list:\n",
    "        if word in freq_u2.keys():\n",
    "            count = freq_u2[word]\n",
    "        else:\n",
    "            count = 0\n",
    "        prob_u2_with_ls.append((count + 1)/(total_cnts_features_u2 + total_features))\n",
    "    u2_dict = dict(zip(new_word_list,prob_u2_with_ls))\n",
    "    u2_prob = training_data.groupby('urgency').count()['keywords'][2]\n",
    "    for keyword in new_word_list:\n",
    "        u2_prob = u2_prob * u2_dict[keyword]\n",
    "\n",
    "    prob_u3_with_ls = []\n",
    "    for word in new_word_list:\n",
    "        if word in freq_u3.keys():\n",
    "            count = freq_u3[word]\n",
    "        else:\n",
    "            count = 0\n",
    "        prob_u3_with_ls.append((count + 1)/(total_cnts_features_u3 + total_features))\n",
    "    u3_dict = dict(zip(new_word_list,prob_u3_with_ls))\n",
    "    u3_prob = training_data.groupby('urgency').count()['keywords'][3]\n",
    "    for keyword in new_word_list:\n",
    "        u3_prob = u3_prob * u3_dict[keyword]\n",
    "\n",
    "    prob_u4_with_ls = []\n",
    "    for word in new_word_list:\n",
    "        if word in freq_u4.keys():\n",
    "            count = freq_u4[word]\n",
    "        else:\n",
    "            count = 0\n",
    "        prob_u4_with_ls.append((count + 1)/(total_cnts_features_u4 + total_features))\n",
    "\n",
    "    u4_dict = dict(zip(new_word_list,prob_u4_with_ls))\n",
    "    u4_prob = training_data.groupby('urgency').count()['keywords'][4]\n",
    "    for keyword in new_word_list:\n",
    "        u4_prob = u4_prob * u4_dict[keyword]\n",
    "\n",
    "\n",
    "    max_prob = max(u1_prob, u2_prob, u3_prob, u4_prob)\n",
    "    if max_prob == u1_prob:\n",
    "        return 1\n",
    "    elif max_prob == u2_prob:\n",
    "        return 2\n",
    "    elif max_prob == u3_prob:\n",
    "        return 3\n",
    "    elif max_prob == u4_prob:\n",
    "        return 4\n",
    "    \n",
    "\n",
    "\n",
    "root= tk.Tk()\n",
    "\n",
    "canvas1 = tk.Canvas(root, width = 200, height = 150)\n",
    "canvas1.pack()\n",
    "\n",
    "entry1 = tk.Entry (root) \n",
    "canvas1.create_window(100, 50, window=entry1)\n",
    "title = tk.Label(root, text= \"Enter ticket sentence to\\n get urgency level!\")\n",
    "canvas1.create_window(100, 20, window=title)\n",
    "    \n",
    "def call_urgency_predictor ():  \n",
    "    x1 = entry1.get()\n",
    "    keys = extract_keywords(x1)\n",
    "    print(\"Keywords: \" + keys)\n",
    "    sentiment = extract_sentiment(x1)\n",
    "    print(\"Sentiment: \" + str(sentiment))\n",
    "    urgency_level = predict_urgency(keys)\n",
    "    print(\"Original urgency: \" + str(urgency_level))\n",
    "    # Sentiment is between 0 and 1, higher score means more positive sentiment. So, higher score means less urgent\n",
    "    urgency_level_final = round(urgency_level + ((1 - sentiment) * 5))\n",
    "    print(\"Urgency with sentiment: \" + str(urgency_level_final))\n",
    "    label1 = tk.Label(root, text= \"Urgency Level: \" + str(urgency_level_final))\n",
    "    canvas1.create_window(100, 120, window=label1)\n",
    "    \n",
    "def enter_button(event):\n",
    "    call_urgency_predictor()\n",
    "    \n",
    "def extract_keywords(sentence):\n",
    "    documents = {\"documents\": [\n",
    "    {\"id\": \"1\", \"language\": \"en\",\n",
    "        \"text\": sentence}]}\n",
    "    \n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
    "    response = requests.post(keyphrase_url, headers=headers, json=documents)\n",
    "    key_phrases = response.json()\n",
    "    \n",
    "    s = \"\"\n",
    "    for j in range(len(response.json()['documents'][0]['keyPhrases'])):\n",
    "        s = s + \" \" + response.json()['documents'][0]['keyPhrases'][j]\n",
    "    if s == \"\":\n",
    "        s = \"NoKeywordsFound\"\n",
    "    return s\n",
    "\n",
    "def extract_sentiment(sentence):\n",
    "    documents = {\"documents\": [\n",
    "    {\"id\": \"1\", \"language\": \"en\",\n",
    "        \"text\": sentence}]}\n",
    "    \n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
    "    response = requests.post(sentiment_url, headers=headers, json=documents)\n",
    "    key_phrases = response.json()\n",
    "    \n",
    "    return response.json()['documents'][0]['score']\n",
    "\n",
    "button1 = tk.Button(text='Get urgency level', command=call_urgency_predictor)\n",
    "root.bind('<Return>', enter_button)\n",
    "canvas1.create_window(100, 80, window=button1)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
